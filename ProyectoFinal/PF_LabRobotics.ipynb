{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universidad de Monterrey \n",
    "## División de Ingenierías\n",
    "### Lab. Robotics\n",
    "#### Proyecto Final \n",
    "Authors: Alejandro Ramirez, Kassandra Ibarra, Alberto Herrera \n",
    "\n",
    "### Introducción \n",
    "\n",
    "\n",
    "### Objetivo \n",
    "El objetivo de este proyecto es lograr diseñar y construir un robot móvil el cual podrá evadir obstáculos, detectar círculos mediante el uso de visión computacional y manipular piezas con el uso de una garrita montada en la parte superior del robot móvil.\n",
    "\n",
    "### Materiales\n",
    "* Raspberry PI 3\n",
    "* Arduino Mega \n",
    "* Sensores infrarrojos y ultrasonico\n",
    "* Garrita manipuladora\n",
    "* Camara de \n",
    "* Bateria de 5200 mA/Hr\n",
    "* Estructura de Carrito Movil (Tanque) \n",
    "* Shield regulador de voltaje \n",
    "* Shield regulador de montor \n",
    "* Pantalla y cable HDMI para visualizar la raspberry.\n",
    "* Además, la Raspberry debe contar con el software pedido al inicio del laboratorio (opencv, python 3.5, Jupyter, ssh, entre otros).\n",
    "\n",
    "### Procedimiento\n",
    "Para realiza este proyecto se tuvo que comenzar programando las máscaras de color y la detección de círculos para poder detectar si el robot móvil se encontraba en la zona de descarga (amarilla), base (azul) además de la detección de piezas rojas.\n",
    "\n",
    "##### Mascaras de color \n",
    "\n",
    "Para comenzar primero se tiene que obtener el HSV de la imagen el cual se compone de (Hue, Saturation, value) para esto se debe poner la siguiente instrucción: hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) en la cual convierte de un marco de RGB a uno HSV, después para crear la máscara de la imagen se debe de utilizar la instrucción:\n",
    "\n",
    "mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "con la cual se creará una imagen binaria en la que se tendrán valores altos de intensidad en los pixeles que se detecte el color deseado, en este caso se realiza la detección del color azul, rojo y amarillo. Despues para poder obtener la imagen con las áreas únicamente del color que deseamos se tiene que realizar una operación AND bitwise con la máscara que se generó anteriormente, las funciones utilizadas para este fin son las siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_blue(hsv):\n",
    "        h_val_l = 80  #Blue\n",
    "        h_val_h = 120 #Blue      \n",
    "        s_val_l = 100\n",
    "        v_val_l = 100\n",
    "        lower_blue = np.array([h_val_l,s_val_l, v_val_l])\n",
    "        upper_blue = np.array([h_val_h, 255, 255])\n",
    "        # threshold the hsv image so that only blue pixels are kept\n",
    "        mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "       # visualise segmented blue object\n",
    "#        cv2.imshow('blue object', blue_object_img)\n",
    "        return(mask)\n",
    "\n",
    "def show_black(hsv):\n",
    "\th_val_l = 0  #Red\n",
    "\th_val_h = 0 #Red\n",
    "\ts_val_l = 0\n",
    "    v_val_l = 0\n",
    "\tlower_red = np.array([h_val_l,s_val_l, v_val_l])\n",
    "\tupper_red = np.array([h_val_h, 255, 255])\n",
    "        # threshold the hsv image so that only blue pixels are kept\n",
    "\tmask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "\n",
    "        # visualise segmented blue object\n",
    "#\tcv2.imshow('red object', red_object_img)\n",
    "\treturn(mask)\n",
    "\t\n",
    "def show_red(hsv):\n",
    "\th_val_l = 160  #Red\n",
    "\th_val_h = 200 #Red\n",
    "\ts_val_l = 100\n",
    "\tv_val_l = 100\n",
    "\tlower_red = np.array([h_val_l,s_val_l, v_val_l])\n",
    "\tupper_red = np.array([h_val_h, 255, 255])\n",
    "        # threshold the hsv image so that only blue pixels are kept\n",
    "\tmask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "\n",
    "\treturn(mask)\n",
    "\n",
    "def show_yellow(hsv):\n",
    "\th_val_l = 20  \n",
    "\th_val_h = 40 \n",
    "\ts_val_l = 100\n",
    "\tv_val_l = 100\n",
    "\tlower_yellow = np.array([h_val_l,s_val_l, v_val_l])\n",
    "\tupper_yellow = np.array([h_val_h, 255, 255])\n",
    "        # threshold the hsv image so that only blue pixels are kept\n",
    "\tmask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "\n",
    "\treturn(mask)\n",
    "\n",
    "def show_violet(hsv):\n",
    "\th_val_l = 130  \n",
    "\th_val_h = 170 \n",
    "\ts_val_l = 50\n",
    "\tv_val_l = 50\n",
    "\tlower_violet = np.array([h_val_l,s_val_l, v_val_l])\n",
    "\tupper_violet = np.array([h_val_h, 255, 255])\n",
    "        # threshold the hsv image so that only blue pixels are kept\n",
    "\tmask = cv2.inRange(hsv, lower_violet, upper_violet)\n",
    "\n",
    "        # AND-bitwise operation between the mask and input images\n",
    "\tviolet_object_img = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "\treturn(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deteccion de circulos \n",
    "Para la detección de círculos utilizamos la función circles = cv2.HoughCircles() el cual funciona mediante el uso del gradiente Hough. El código en donde se detecta el círculo es el siguiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_circ(cimg,frame):\n",
    "\tcimg = cv2.GaussianBlur(cimg,(7\t,7),0)\n",
    "\tkernel = np.ones((5,5),np.uint8)\n",
    "\tcircles = cv2.HoughCircles(cimg,cv2.HOUGH_GRADIENT,1,10,param1=50,param2=25,minRadius=5,maxRadius=0)\n",
    "\tif circles is not None:\n",
    "\t\tcircles = np.around(circles)\n",
    "\n",
    "\t\tfor i in circles[0,:]:\n",
    "\t\t\t# draw the outer circle\n",
    "\t\t\tcv2.circle(frame,(i[0],i[1]),i[2],(0,255,0),2)\n",
    "\t\t\t# draw the center of the circle\n",
    "\t\t\tcv2.circle(frame,(i[0],i[1]),2,(0,0,255),3)\n",
    "\t\treturn(frame,i[0],i[1])\n",
    "\telse:\n",
    "\t\treturn(frame,0,0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función se utiliza en la función ROI(mask,x,y,w,h) que se encarga de verificar si existe un círculo dentro de la región de interés calculada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def\tROI(framex,x,y,w,h):\n",
    "\tif x<5:\n",
    "\t\tx=5\n",
    "\tif y<5:\n",
    "\t\ty=5\n",
    "\timg_roi = framex[y-5:y+w+5,x-5:x+h+5]\n",
    "\t#show('ROI',img_roi)\n",
    "\ta,b,c=det_circ(img_roi,framex)\n",
    "\t#show(\"circulos\",a)\n",
    "\tglobal disp\n",
    "\tdisp= False\n",
    "\tif b>0 and c>0:\n",
    "\t\tdisp = True\n",
    "\telse: \n",
    "\t\tdisp =False\n",
    "\treturn(disp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ya que fue explicado el propósito de las funciones utilizadas se presenta la función encargada de encontrar el centro del círculo detectado el cual utiliza la función ROI(framex,x,y,w,h) para verificar que si existe el círculo y que esté a su vez utiliza la función det_circ(cimg,frame) para encontrar el círculo. en caso de que la función ROI(framex,x,y,w,h) determine que sí existe el círculo la función cnt(mask,framex) calculará las coordenadas del centro del círculo mediante el uso de la instrucción cv2.moments(cont) el cual nos proporciona los parámetros necesarios para poder realizar las operaciones ya determinadas para calcular Cx y Cy.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt(mask,framex):\n",
    "\tcX = 0\n",
    "\tcY = 0\n",
    "\tframet, contours, hierarchy = cv2.findContours(mask,3,2)\n",
    "\t#cv2.imshow('framet', framet)\n",
    "\tnum = len(contours)\n",
    "\t\n",
    "\tfor cont in contours:\n",
    "\t\t\n",
    "\t\trect = cv2.minAreaRect(cont)\n",
    "\t\tbox = cv2.boxPoints(rect)\n",
    "\t\t#bo0x2 = box\n",
    "\t\tbox = np.int0(box)\n",
    "\t\t(x, y, w, h) = cv2.boundingRect(cont)\n",
    "\t\tdisp = ROI(mask,x,y,w,h)\n",
    "\t\t#print (disp)\n",
    "\t\tif disp:\n",
    "\t\t\tcv2.drawContours(framex, [box], 0, (0,255,200), 3)\n",
    "\t\t\tif h >10 and w >10:\n",
    "\n",
    "\t\t\t\tM = cv2.moments(cont)\n",
    "\t\t\t\tif M[\"m00\"]>0:\n",
    "\t\t\t\n",
    "\t\t\t\t\tcX = int(M[\"m10\"] / M[\"m00\"])\n",
    "\t\t\t\t\tcY = int(M[\"m01\"] / M[\"m00\"])\n",
    "\t\t\t\t\tcv2.circle(framex, (cX, cY), 1, (255, 255, 255), -1)\n",
    "\t\t\t\t\tcv2.putText(framex, \"center\", (cX - 20, cY - 20),\n",
    "\t\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "\tif cX == 0 and cY == 0:\n",
    "\t\treturn(framex, 0, 0)\n",
    "\telse:\n",
    "\t\t\n",
    "\t\tprint(\"\\n\\n\\n\")\n",
    "\t\tprint(cX,cY)\n",
    "\t\tprint(\"\\n\\n\\n\")\n",
    "\t\treturn(framex,cX,cY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Navegación \n",
    "En la programación de la navegación se realizaron varias funciones para determinar la dirección mediante el sensado del ambiente con el uso de sensores y el procesamiento de la cámara utilizada.\n",
    "\n",
    "###### Sensores Opticos y Ultrasonico \n",
    "Para poder procesar la información de los sensores Ópticos y Ultrasonido fue necesario utilizar el Arduino Mega ya que el Raspberry Pi no cuenta con las entradas analogicas necesarias para utilizar los sensores Ópticos, para poder organizar mejor la información decidimos dejarle la responsabilidad al arduino de procesar tanto los sensores ópticos como el ultrasónico por lo que el código utilizados en el Raspberry Pi abarca principalmente el procesamiento de la comunicación serial establecida con el arduino. También se debe mencionar que se utilizó la mediana de los valores proporcionados para así evitar valores atípicos o extremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_sens():\n",
    "\tser.flushInput()\n",
    "\tser.flushOutput()\n",
    "\tprom0=[]\n",
    "\tprom1=[]\n",
    "\tus=[]\n",
    "\tsens=[]\n",
    "\tfor i in range(1,11):\n",
    "\t\tser.write(b's')\n",
    "\t\twhile(len(sens)<3):\n",
    "\t\t\tstate = ser.readline()\n",
    "\t\t\tstate=state.decode(\"utf-8\") \n",
    "\t\t\tstate=state[0:len(state)-3]\n",
    "\t\t\tsens= state.split(\" \",3)\n",
    "\t\ttry:\t\n",
    "\t\t\tprom0.append(int(sens[0]))\n",
    "\t\texcept:\n",
    "\t\t\tprom0.append(0)\n",
    "\t\ttry:\t\n",
    "\t\t\tprom1.append(int(sens[1]))\n",
    "\t\texcept:\n",
    "\t\t\tprom1.append(0)\n",
    "\t\ttry:\t\n",
    "\t\t\tus.append(int(sens[2]))\n",
    "\t\texcept:\n",
    "\t\t\tus.append(0)\n",
    "\tprom0.sort()\n",
    "\tprom1.sort()\n",
    "\tus.sort()\n",
    "\treturn (prom0[6], prom1[6], us[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Procesamiento de Imagen \n",
    "Además de la lectura de sensores se necesito el procesamiento visual proporcionado por la cámara para poder detectar las piezas y zonas de base y descarga colocados. Para esto se desarrolló otra función llamada leer_img(busca='pieza') la cual se encarga de realizar todo el análisis de la imagen. Comenzando con el procesamiento para determinar que color de círculo debe de buscar dependiendo si se busca la base, la descarga o la pieza a recoger mediante el uso de las funciones ya explicadas al inicio del reporte para crear las máscaras de colores, después se utiliza la función cnt(mask,framex) encargada de la detección de las coordenadas del centro del círculo mediante la detección de círculos facilitada por las funciones ya explicadas en detección de círculos. Ya que contamos con toda esta información realizamos la comparación de distancias entre el centro del círculo y la coordenada del centro de la imagen de la cámara para poder determinar la dirección de giro necesario para centrar el círculo.Todo esto se encuentra en la función a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_img(busca='pieza'):\n",
    "\tn=1\n",
    "\twhile(n>0):\n",
    "\t\tret, frame = cap.read()\n",
    "\t\tframe = cv2.resize(frame, (240, 180)) \n",
    "\t\tframex = frame\n",
    "\t\t#cv2.imshow('framex', framex)\n",
    "\t\t\n",
    "\t\thsv = cv2.cvtColor(framex, cv2.COLOR_BGR2HSV)\n",
    "\t\t\t# ----- objects can be detected                    ------ #\n",
    "\t\tif busca == 'base' :\n",
    "\t\t\tmask = show_blue(hsv)\n",
    "\t\t\tcap.set(11,.8)\n",
    "\t\t\tcap.set(10,1)\n",
    "\t\telif busca== 'descarga':\n",
    "\t\t\tmask = show_yellow(hsv)\n",
    "\t\t\tcap.set(11,.5)\n",
    "\t\t\tcap.set(10,.3)\n",
    "\t\t\tcap.set(12,.5)\t\n",
    "\t\telif busca == 'pieza':\n",
    "\t\t\tmask = show_red(hsv)\n",
    "\t\t\tcap.set(11,.8)\n",
    "\t\t\tcap.set(10, 1)\n",
    "\t\t\tcap.set(12,1)\n",
    "\t\t\t#mask = mask + show_violet(hsv)\n",
    "\t\t# ------------------------------------------------------- #\n",
    "\t\tmaskx = cv2.medianBlur(mask,7)\n",
    "\t\tkernel = np.ones((5,5),np.uint8)\n",
    "\t\tf_filter = cv2.bitwise_and(frame, frame, mask=maskx)\n",
    "\t\t#visualise current frame\n",
    "\t\n",
    "\t\t#visualise mask imagec\n",
    "\t\tcv2.imshow('mask', maskx)\n",
    "\t\tframe,cx,cy = cnt(maskx,frame)\t\n",
    "\t\t#frame,cx,cy = det_circ(f_filter,frame)\t\n",
    "\t\tmov='None'\n",
    "\t\tif cx>=115 and cx<=125 :\n",
    "\t\t\tbox_color = (0,255,0)\n",
    "\t\t\tpix=0\n",
    "\t\t\tmov = 'cent'\n",
    "\t\telif cx==0 and cy==0:\n",
    "\t\t\tbox_color = (255,255,255)\n",
    "\t\t\tmov = 'None'\n",
    "\t\t\tpix=0\n",
    "\t\telse:\n",
    "\t\t\tif cx < 115:\n",
    "\t\t\t\tpix = 115-cx\n",
    "\t\t\t\tmov='der'\n",
    "\t\t\t\t#print('Izq')\n",
    "\t\t\telif cx>125:\n",
    "\t\t\t\tpix = cx-125\n",
    "\t\t\t\tmov='izq'\n",
    "\t\t\t\t#print('Der')\t\n",
    "\t\t\telse:\n",
    "\t\t\t\tmov = 'det'\n",
    "\t\t\t\tpix=0\t\t\n",
    "\t\t\tbox_color = (0,0,255)\n",
    "\t\tboxing(frame,box_color);\n",
    "\t\t#print(cx,cy)\n",
    "\t\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\t\tbreak \n",
    "\t\t\t\n",
    "\t\tcv2.imshow('frame',frame)\n",
    "\t\tn=n-1\n",
    "\treturn(mov, pix)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez realizadas estas funciones ya se puede realizar el algoritmo de navegación que se muestra a continuación en el cual la primera parte se encarga de la búsqueda de pieza mediante el uso de funciones como leer_img(busca) mientras que en la última parte se encarga de evadir obstáculos detectados en la función leer_sens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAV():\n",
    "\t\n",
    "\tglobal lastmov\n",
    "\tglobal busca\n",
    "\tglobal sec\n",
    "\tglobal cont\n",
    "\tprom0, prom1, us= leer_sens()\n",
    "\t#print(prom1)\n",
    "\t#print(prom0)\n",
    "\t#print(us)\n",
    "\t#print(\"out while\")\n",
    "\tif sec==1:\n",
    "\t\ttime.sleep(.001)\n",
    "\t\tcont=cont+1\n",
    "\t\tprint(cont)\n",
    "\telse:\n",
    "\t\tcont=0\n",
    "\tif cont >=1000:\n",
    "\t\tbusca ='base'\n",
    "\t\tsec=3\n",
    "\t\tcont=0\n",
    "\tcirc=False\n",
    "\tmov, pix=leer_img(busca)\n",
    "\twhile(((prom0<310 and prom1<310) and us > 11) or disp):\n",
    "\t\tif sec==1:\n",
    "\t\t\ttime.sleep(.001)\n",
    "\t\t\tcont=cont+1\n",
    "\t\t\tprint(cont)\n",
    "\t\telse :\n",
    "\t\t\tcont=0\n",
    "\t\tif cont >=1000:\n",
    "\t\t\tbusca ='base'\n",
    "\t\t\tsec=3\n",
    "\t\t\tcont=0\n",
    "\t\tprint(\"\\n\\n----Sensores----\")\n",
    "\t\tprint(prom0,prom1,us)\n",
    "\t\tprint(\"\\n\\n\")\n",
    "\t\tprint(busca)\n",
    "\t\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\t\tbreak \n",
    "\t\tmov,pix= leer_img(busca)\n",
    "\t\tdelay = 1.5*pix+.1*(pix+pix_A)/2\n",
    "\t\t#print(delay,'DELAY')\n",
    "\t\tif delay > 5:\n",
    "\t\t\tdelay = 5\n",
    "\t\tif mov == 'izq' :\n",
    "\t\t\tcont=0\n",
    "\t\t\tp.ChangeDutyCycle(delay)\n",
    "\t\t\tp2.ChangeDutyCycle(delay)\t\n",
    "\t\t\tleft()\n",
    "\t\t\ttime.sleep(.015)\n",
    "\t\t\t#forward()\n",
    "\t\t\tstop()\t\n",
    "\t\t\tprint('\\n\\nIZQ\\n\\n')\n",
    "\t\telif mov == 'der':\n",
    "\t\t\tcont=0\n",
    "\t\t\tp.ChangeDutyCycle(delay)\n",
    "\t\t\tp2.ChangeDutyCycle(delay)\t\n",
    "\t\t\tright()\n",
    "\t\t\ttime.sleep(.015)\n",
    "\t\t\tstop()\n",
    "\t\t\tprint('\\n\\nDER\\n\\n')\n",
    "\t\telif mov == 'cent':\n",
    "\t\t\tcont=0\n",
    "\t\t\tp.ChangeDutyCycle(15)\n",
    "\t\t\tp2.ChangeDutyCycle(15)\n",
    "\t\t\tforward()\n",
    "\t\t\ttime.sleep(.015)\n",
    "\t\t\tstop()\n",
    "\t\t\tprint(prom0, prom1 ,us)\n",
    "\t\t\tif (((us < 30 and (sec ==2 or sec==3)) or (us <=16 and sec == 1))):\n",
    "\t\t\t\tprint(\"Sensores dentro de sec\")\n",
    "\t\t\t\tprint(prom0, prom1, us)\n",
    "\t\t\t\tif sec ==1: #and us <= 11:\n",
    "\t\t\t\t\tforward()\n",
    "\t\t\t\t\ttime.sleep(.6)\n",
    "\t\t\t\t\tstop()\n",
    "\t\t\t\t\tprint 'in sec1'\n",
    "\t\t\t\t\tser.write(b'1')\n",
    "\t\t\t\t\tbusca='descarga'\n",
    "\t\t\t\t\tsec=2\n",
    "\t\t\t\telif sec == 2:\n",
    "\t\t\t\t\tser.write(b'2')\n",
    "\t\t\t\t\tsec=1\n",
    "\t\t\t\t\tbusca='pieza'\n",
    "\t\t\t\telif sec == 3 :\n",
    "\t\t\t\t\texit()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tprint(\"secuencia\")\n",
    "\t\t\t\tprint(sec)\n",
    "\t\t\t\ttime.sleep(10)\n",
    "\t\t\t\t#send=False\n",
    "\t\t\t\tleer_img(busca)\n",
    "\t\t\t\t\t\t\n",
    "\t\telse :\n",
    "\t\t\tp.ChangeDutyCycle(15)\n",
    "\t\t\tp2.ChangeDutyCycle(15)\n",
    "\t\t\tforward()\n",
    "\t\t\t\t\t\n",
    "\t\tprint(\"DELAY\")\n",
    "\t\tprint(delay)\t\t\n",
    "\t\t#time.sleep(delay)\n",
    "\t\tpix_A=pix\n",
    "\t\tprom0,prom1, us=leer_sens()\n",
    "\t\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\t\tbreak \n",
    "\t\tlastmov = mov;\n",
    "#\tbackward()\n",
    "#\tleft()\n",
    "\tstop()\n",
    "\t\n",
    "\tif ((prom0>=310 or prom1>=310 or us<11) and not disp):\n",
    "\t\tp2.ChangeDutyCycle(10)\n",
    "\t\tp.ChangeDutyCycle(10)\n",
    "\t\tleer_img()\n",
    "\t\tprom0,prom1,us=leer_sens()\n",
    "\t\tif disp:\n",
    "\t\t\tif prom0>prom1:\n",
    "\t\t\t\tright()\n",
    "\t\t\telse:\n",
    "\t\t\t\tleft()\n",
    "\t\tif lastmov == 'izq':\n",
    "\t\t\tright()\n",
    "\t\telif lastmov == 'der':\n",
    "\t\t\tleft()\n",
    "\t\telif lastmov == 'cent':  \n",
    "\t\t\tforward()\n",
    "\t\telse: \n",
    "\t\t\tleft()\n",
    "\t\ttime.sleep(.02)\n",
    "\t\tstop()\n",
    "\t\t\n",
    "\treturn(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se definieron funciones en las que se definió el movimiento de los motores del carrito para poder realizar el giro necesario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward():\n",
    "\n",
    "\n",
    "     GPIO.output(RIGHTF, GPIO.HIGH)\n",
    "\n",
    "     GPIO.output(RIGHTB, GPIO.LOW)\n",
    "\n",
    "     GPIO.output(LEFTF, GPIO.HIGH)\n",
    "\n",
    "     GPIO.output(LEFTB, GPIO.LOW)\n",
    "\n",
    "     print(\"FORWARD\")\n",
    "\n",
    "def backward():\n",
    "\n",
    "     time.sleep(0)\n",
    "\n",
    "     GPIO.output(RIGHTF, GPIO.LOW)\n",
    "\n",
    "     GPIO.output(RIGHTB, GPIO.HIGH)\n",
    "\n",
    "     GPIO.output(LEFTF, GPIO.LOW)\n",
    "\n",
    "     GPIO.output(LEFTB, GPIO.HIGH)\n",
    "\n",
    "     print(\"BACKWARD\")\n",
    "\n",
    "\n",
    "def stop():\n",
    "\n",
    "     GPIO.output(RIGHTF, GPIO.LOW)\n",
    "\n",
    "     GPIO.output(RIGHTB, GPIO.LOW)\n",
    "\n",
    "     GPIO.output(LEFTF, GPIO.LOW)\n",
    "\n",
    "     GPIO.output(LEFTB, GPIO.LOW)\n",
    "     \n",
    "     print(\"STOP\")\n",
    " \n",
    "\n",
    "def right():\n",
    "     GPIO.output(RIGHTF, GPIO.HIGH)\n",
    "\n",
    "     GPIO.output(RIGHTB, GPIO.LOW)\n",
    "\n",
    "     GPIO.output(LEFTF, GPIO.LOW)\n",
    "\n",
    "     GPIO.output(LEFTB, GPIO.HIGH)\n",
    "\n",
    "     print(\"RIGHT\")\n",
    "     \n",
    "     \n",
    "\n",
    "def left():\n",
    "\n",
    "     GPIO.output(RIGHTF, GPIO.LOW)\n",
    "\n",
    "     GPIO.output(RIGHTB, GPIO.HIGH)\n",
    "\n",
    "     GPIO.output(LEFTF, GPIO.HIGH)\n",
    "\n",
    "     GPIO.output(LEFTB, GPIO.LOW)\n",
    "\n",
    "     print(\"LEFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El código main consta de un ciclo infinito de la función de Navegación como se muestra a continuación \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_global()\n",
    "while(1):\n",
    "\tNAV()\n",
    "\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\tbreak "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "#### Jesus Alejandro Ramirez Castañeda\n",
    "\n",
    "#### Kassandra Dzuara Ibarra Ortiz \n",
    "Este proyecto se describe con una palabra, retador. Ya que no solo aplicamos el conocimiento que adquirimos en clase si no que aplicamos mucho más. Aunque logramos realizar el programa necesario para que funcionara correctamente tuvimos muchos problemas al momento de realizar la implementación física ya que la garrita era muy corta y débil, y los sensores tienen un alto margen de error el cual \"confunde \" al robot a pesar de que la programación era la adecuada. Creo que lo que podemos mejorar es principalmente el estilo de Navegación ya que con el estilo implementado no podía alcanzar todas las zonas deseadas así como cambiar la garra por una que tuviera más libertad de movimiento y alcance. Aun y con estas áreas de oportunidad creo que realizamos un buen trabajo en equipo así como un buen programa que es capaz de realizar una secuencia y deteccion de obstaculos impecable si no se toman en cuenta las limitaciones físicas de los sensores. Me llevó mucho por aprender pero a la vez me siento satisfecha por el trabajo que hicimos el cual un semestre anterior habría sido más que complejo, imposible. \n",
    "\n",
    "#### Alberto Jasiel Herrera Michel \n",
    " Este proyecto nos impulsó a aprender más contenido del que vimos en clase y aparte lo pusimos en práctica; si bien el resultado del proyecto no fue el óptimo, creo que hay que tomar en cuenta que nuestro proyecto contaba con una parte de implementación física aparte de la programación, pero esto no es ni excusa ni justificación. Creo que la visión computacional es un área muy interesante y que es una manera sencilla de implementar a un sistema cierto grado de inteligencia; en semestres pasados estuvimos trabajando en un robot móvil que navegaba, para ese utilizamos solo sensores para que navegara y si lo comparamos con este proyecto este es mil veces más \"inteligente\". \n",
    "  Creo que uno de los grandes errores que cometimos en el proyecto fue que las pruebas las llevamos a cabo siempre en un ambiente controlado en la que del modo en que colocábamos las zonas siempre lográbamos los objetivos. Creo que nos faltó programar una navegación más dinámica previendo todos los escenarios posibles. Aun así, me siento satisfecho con el trabajo realizado, pues requirió mucho trabajo y tiempo y no nos limitamos a lo que ya conocíamos.\n",
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
